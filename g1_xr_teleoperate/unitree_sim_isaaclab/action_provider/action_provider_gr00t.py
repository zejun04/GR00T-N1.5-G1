# /home/shenlan/GR00T-VLA/g1_xr_teleoperate/unitree_sim_isaaclab/action_provider/action_provider_gr00t.py

import numpy as np
import requests
import json
import time
from typing import Dict, Any, Tuple
import cv2
import sys
import os
import torch
import logging

try:
    import json_numpy
    json_numpy.patch()
    JSON_NUMPY_AVAILABLE = True
    print("âœ… json_numpy configured successfully")
except ImportError:
    JSON_NUMPY_AVAILABLE = False
    print("âŒ json_numpy not available, using fallback serialization")

class GR00TActionProvider:
    def __init__(self, env, args):
        """
        GR00TåŠ¨ä½œæä¾›å™¨
        """
        self.env = env
        self.args = args
        self.host = getattr(args, 'gr00t_host', 'localhost')
        self.port = getattr(args, 'gr00t_port', 8000)
        
        # å¿…éœ€çš„å±æ€§
        self.name = "GR00TActionProvider"
        
        # è®¾ç½®é»˜è®¤å›¾åƒå°ºå¯¸
        self.image_size = (640, 480)  # GR00TæœŸæœ›çš„é»˜è®¤å°ºå¯¸(640,480)
        
        # åŠ¨ä½œåºåˆ—ç›¸å…³å±æ€§
        self.action_sequence = None
        self.current_step = 0
        self.sequence_length = 16  # GR00Tè¿”å›çš„åºåˆ—é•¿åº¦
        self.last_sequence_time = 0
        self.sequence_request_interval = 2.0  # æ¯2ç§’è¯·æ±‚æ–°åºåˆ—
        
        # æ£€æŸ¥ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´
        self._check_action_space()
        
        # åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯
        self.session = requests.Session()
        self.base_url = f"http://{self.host}:{self.port}"
        
        # æµ‹è¯•è¿æ¥
        self._test_connection()
        
        print("âœ… GR00T Action Provider initialized successfully")
    
    def _check_action_space(self):
        """æ£€æŸ¥ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´"""
        try:
            if hasattr(self.env, 'action_space'):
                print(f"ğŸŸ¡ ç¯å¢ƒåŠ¨ä½œç©ºé—´: {self.env.action_space}")
                print(f"ğŸŸ¡ åŠ¨ä½œç©ºé—´å½¢çŠ¶: {self.env.action_space.shape}")
                print(f"ğŸŸ¡ åŠ¨ä½œç©ºé—´ç±»å‹: {type(self.env.action_space)}")
                
                # è·å–åŠ¨ä½œç©ºé—´çš„æ­£ç¡®ç»´åº¦
                if hasattr(self.env.action_space, 'shape'):
                    # åŠ¨ä½œç©ºé—´å½¢çŠ¶æ˜¯ (1, 43)ï¼Œæˆ‘ä»¬éœ€è¦43ç»´çš„åŠ¨ä½œ
                    if len(self.env.action_space.shape) == 2:
                        self.action_dim = self.env.action_space.shape[1]  # è·å–43
                        self.action_shape = (1, self.action_dim)  # ä¿å­˜å®Œæ•´çš„å½¢çŠ¶
                    else:
                        self.action_dim = self.env.action_space.shape[0]
                        self.action_shape = (self.action_dim,)
                    print(f"ğŸŸ¡ åŠ¨ä½œç»´åº¦: {self.action_dim}, åŠ¨ä½œå½¢çŠ¶: {self.action_shape}")
                else:
                    print("âš ï¸ æ— æ³•è·å–åŠ¨ä½œç©ºé—´å½¢çŠ¶ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦")
                    self.action_dim = 43
                    self.action_shape = (1, 43)
            else:
                print("âš ï¸ ç¯å¢ƒæ²¡æœ‰action_spaceå±æ€§ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦")
                self.action_dim = 43
                self.action_shape = (1, 43)
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥åŠ¨ä½œç©ºé—´æ—¶å‡ºé”™: {e}")
            self.action_dim = 43
            self.action_shape = (1, 43)
        
    
    def _test_connection(self):
        """æµ‹è¯•ä¸GR00TæœåŠ¡çš„è¿æ¥"""
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=5)
            if response.status_code == 200:
                print("âœ… Successfully connected to GR00T inference service")
            else:
                print(f"âš ï¸ GR00T service returned status: {response.status_code}")
        except Exception as e:
            print(f"âŒ Failed to connect to GR00T service: {e}")
            print("Please make sure GR00T inference service is running:")
            print("python scripts/inference_service.py --server --http-server --port 8000 --embodiment_tag gr1 --data_config so100")
            raise e
    
    def start(self):
        """å¯åŠ¨åŠ¨ä½œæä¾›å™¨"""
        print("ğŸŸ¢ GR00T Action Provider started")
    
    def stop(self):
        """åœæ­¢åŠ¨ä½œæä¾›å™¨"""
        print("ğŸ”´ GR00T Action Provider stopped")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.close()
    
    def get_action(self, env=None):
        """
        ä»GR00TæœåŠ¡è·å–åŠ¨ä½œ
        
        Args:
            env: ä»¿çœŸç¯å¢ƒï¼ˆä¸ºäº†æ¥å£å…¼å®¹æ€§ï¼‰
            
        Returns:
            torch.Tensor: åŠ¨ä½œå¼ é‡ï¼Œç¬¦åˆä»¿çœŸç¯å¢ƒæœŸæœ›çš„æ ¼å¼
        """
        # å¦‚æœä¼ å…¥äº†æ–°çš„envï¼Œæ›´æ–°å½“å‰env
        if env is not None:
            self.env = env
        try:
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–æ–°çš„åŠ¨ä½œåºåˆ—
            if (self.action_sequence is None or 
                self.current_step >= self.sequence_length):
                
                # print("ğŸ”„ è·å–æ–°çš„åŠ¨ä½œåºåˆ—...")
                self.action_sequence = self._get_new_action_sequence()
                self.current_step = 0
                self.last_sequence_time = current_time

            # ä»åºåˆ—ä¸­æå–å½“å‰æ­¥éª¤çš„åŠ¨ä½œ
            current_action = self._extract_step_action(self.action_sequence, self.current_step)
            self.current_step += 1
            
            # å°†åŠ¨ä½œè½¬æ¢ä¸ºä»¿çœŸç¯å¢ƒæœŸæœ›çš„æ ¼å¼
            action_tensor = self._convert_to_env_action(current_action)
            return action_tensor
                
        except Exception as e:
            print(f"âŒ ä»GR00Tè·å–åŠ¨ä½œæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return self._get_default_action()
    
    def _get_new_action_sequence(self) -> Dict[str, np.ndarray]:
        """ä»GR00TæœåŠ¡è·å–å®Œæ•´çš„åŠ¨ä½œåºåˆ—"""
        try:
            # å‡†å¤‡è§‚æµ‹æ•°æ®
            observation = self.prepare_observation()
            print("è§‚æµ‹ï¼š",observation)
            
            # ä½¿ç”¨json_numpyåºåˆ—åŒ–
            json_data = json_numpy.dumps({"observation": observation})
            headers = {'Content-Type': 'application/json'}
            data = json_data
            # print("data: ",data)
            # å‘é€è¯·æ±‚åˆ°GR00TæœåŠ¡
            start_time = time.time()
            response = self.session.post(
                f"{self.base_url}/act",
                data=data,
                headers=headers,
                timeout=10.0
            )
            
            if response.status_code == 200:
                # è§£æå“åº”
                try:
                    action_data = json_numpy.loads(response.text)
                    # print("ä½¿ç”¨json_numpyè§£æå“åº”æˆåŠŸ")
                except Exception as e:
                        print(f"âŒ ä½¿ç”¨json_numpyè§£æå“åº”å¤±è´¥: {e}")
                
                inference_time = time.time() - start_time
                print(f"âœ… GR00Tæ¨ç†æˆåŠŸ - æ—¶é—´: {inference_time:.3f}s")
                
                # éªŒè¯åŠ¨ä½œåºåˆ—ç»“æ„
                # print("åŠ¨ä½œæ˜¯ï¼š",action_data)
                return action_data
            else:
                print(f"âŒ GR00TæœåŠ¡è¿”å›é”™è¯¯: {response.status_code} - {response.text}")
                
        except Exception as e:
            print(f"âŒ è·å–åŠ¨ä½œåºåˆ—å¤±è´¥: {e}")
    
    
    def _extract_step_action(self, action_sequence: Dict[str, np.ndarray], step_idx: int) -> Dict[str, np.ndarray]:
        """
        ä»åŠ¨ä½œåºåˆ—ä¸­æå–æŒ‡å®šæ­¥éª¤çš„åŠ¨ä½œ
        
        Args:
            action_sequence: å®Œæ•´çš„åŠ¨ä½œåºåˆ—
            step_idx: è¦æå–çš„æ­¥éª¤ç´¢å¼•
            
        Returns:
            Dict[str, np.ndarray]: å½“å‰æ­¥éª¤çš„åŠ¨ä½œ
        """
        current_action = {}
        
        for key, sequence in action_sequence.items():
            if isinstance(sequence, np.ndarray) and len(sequence.shape) == 2:
                # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                if step_idx < sequence.shape[0]:
                    current_action[key] = sequence[step_idx]
                else:
                    # å¦‚æœè¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨æœ€åä¸€æ­¥
                    current_action[key] = sequence[-1]
                    print(f"âš ï¸ æ­¥éª¤ç´¢å¼• {step_idx} è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨æœ€åä¸€æ­¥")
            else:
                current_action[key] = sequence
        print("åŠ¨ä½œæ˜¯ï¼š",current_action)
        return current_action
    
    
    def _convert_to_env_action(self, action_data: Dict[str, np.ndarray]) -> torch.Tensor:
        """
        å°†GR00Tè¿”å›çš„åŠ¨ä½œå­—å…¸è½¬æ¢ä¸ºä»¿çœŸç¯å¢ƒæœŸæœ›çš„torch.Tensoræ ¼å¼
        
        Args:
            action_data: GR00Tè¿”å›çš„åŠ¨ä½œå­—å…¸
            
        Returns:
            torch.Tensor: ä»¿çœŸç¯å¢ƒæœŸæœ›çš„åŠ¨ä½œå¼ é‡
        """
        try:
            # å°†åŠ¨ä½œå­—å…¸è½¬æ¢ä¸ºå®Œæ•´43ç»´åŠ¨ä½œå‘é‡
            action_vector = self._build_full_action_vector(action_data)
            
            
            # ç¡®ä¿åŠ¨ä½œç»´åº¦
            action_vector = self._ensure_action_dimension(action_vector)
            
            # é‡å¡‘ä¸ºç¯å¢ƒæœŸæœ›çš„å½¢çŠ¶
            action_vector = action_vector.reshape(self.action_shape)
            
            # è½¬æ¢ä¸ºtorchå¼ é‡
            action_tensor = torch.from_numpy(action_vector).to(self.env.device)
            
            return action_tensor
            
        except Exception as e:
            print(f"âŒ è½¬æ¢åŠ¨ä½œæ ¼å¼æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return self._get_default_action()
        
    
    def _build_full_action_vector(self, action_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """
        æ„å»ºå®Œæ•´çš„43ç»´åŠ¨ä½œå‘é‡
        
        æ ¹æ®G129æœºå™¨äººçš„å…³èŠ‚é¡ºåºï¼š
        - èº«ä½“å…³èŠ‚ï¼š29ä¸ªï¼ˆè…¿éƒ¨12 + è…°éƒ¨3 + æ‰‹è‡‚14ï¼‰
        - æ‰‹éƒ¨å…³èŠ‚ï¼š14ä¸ª
        """
        try:
            # åˆ›å»º43ç»´çš„é›¶å‘é‡
            full_action = np.zeros(43, dtype=np.float32)
            
            # æ˜ å°„GR00TåŠ¨ä½œåˆ°å®Œæ•´çš„å…³èŠ‚ç©ºé—´[å·¦é—­ï¼Œå³å¼€)
            # action.left/right_arm/hand is provided by GR00T
            # index,example 15-21 is used for isaac_sim
            action_mappings = [
                ('action.left_arm', 15, 22),    # å·¦è‡‚ -> ç´¢å¼•15-21
                ('action.left_hand', 29, 36),   # å³è‡‚ -> ç´¢å¼•22-28
                ('action.right_arm', 22, 29),   # å·¦æ‰‹ -> ç´¢å¼•29-35
                ('action.right_hand', 36, 43)   # å³æ‰‹ -> ç´¢å¼•36-42
            ]
            
            if hasattr(self, '_debug_count'):
                self._debug_count += 1
            else:
                self._debug_count = 0

            used_indices = 0
            for key, start_idx, end_idx in action_mappings:
                if key in action_dict:
                    action_part = action_dict[key]
                    dim = end_idx - start_idx
                    
                    if action_part.shape[0] == dim:
                        
                        full_action[start_idx:end_idx] = action_part
                        used_indices += dim
                        # if self._debug_count % 10 == 0:
                        #     print(f"âœ… {key}:")
                        #     print(f"   åŠ¨ä½œèŒƒå›´: [{action_part.min():.4f}, {action_part.max():.4f}]")
                    else:
                        print(f"âš ï¸ åŠ¨ä½œéƒ¨åˆ† {key} ç»´åº¦ä¸åŒ¹é…: {action_part.shape[0]} != {dim}")
                        # ä½¿ç”¨é›¶å‘é‡æ›¿ä»£
                        full_action[start_idx:end_idx] = np.zeros(dim, dtype=np.float32)
                else:
                    print(f"âš ï¸ ç¼ºå°‘åŠ¨ä½œéƒ¨åˆ†: {key}")
                    # ä½¿ç”¨é›¶å‘é‡å¡«å……
                    full_action[start_idx:end_idx] = np.zeros(dim, dtype=np.float32)
            
            
            # å¯¹äºæœªæ˜ å°„çš„éƒ¨åˆ†ï¼ˆè…¿éƒ¨ã€è…°éƒ¨ç­‰ï¼‰ï¼Œä¿æŒä¸ºé›¶
            # è¿™äº›éƒ¨åˆ†å°†ç”±ä»¿çœŸç¯å¢ƒå¤„ç†æˆ–ä¿æŒé»˜è®¤ä½ç½®
            print("G1_action",full_action)
            return full_action
            
        except Exception as e:
            print(f"âŒ æ„å»ºå®Œæ•´åŠ¨ä½œå‘é‡æ—¶å‡ºé”™: {e}")
            # è¿”å›é›¶å‘é‡
            return np.zeros(43, dtype=np.float32)
    
    def _ensure_action_dimension(self, action_vector: np.ndarray) -> np.ndarray:
        """
        ç¡®ä¿åŠ¨ä½œå‘é‡ä¸ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´ç»´åº¦åŒ¹é…
        """
        current_dim = action_vector.shape[0]
        
        if current_dim == self.action_dim:
            # ç»´åº¦åŒ¹é…ï¼Œç›´æ¥è¿”å›
            return action_vector
        elif current_dim > self.action_dim:
            # åŠ¨ä½œå‘é‡ç»´åº¦å¤ªå¤§ï¼Œæˆªæ–­
            print(f"âš ï¸ åŠ¨ä½œå‘é‡ç»´åº¦è¿‡å¤§ ({current_dim} > {self.action_dim})ï¼Œè¿›è¡Œæˆªæ–­")
            return action_vector[:self.action_dim]
        else:
            # åŠ¨ä½œå‘é‡ç»´åº¦å¤ªå°ï¼Œå¡«å……é›¶
            print(f"âš ï¸ åŠ¨ä½œå‘é‡ç»´åº¦è¿‡å° ({current_dim} < {self.action_dim})ï¼Œè¿›è¡Œé›¶å¡«å……")
            padded_action = np.zeros(self.action_dim, dtype=np.float32)
            padded_action[:current_dim] = action_vector
            return padded_action
    
    def prepare_observation(self) -> Dict[str, Any]:
        """
        å‡†å¤‡GR00Tæ¨¡å‹æ‰€éœ€çš„è§‚æµ‹æ•°æ®
        
        Returns:
            Dict: ç¬¦åˆGR00Tè¾“å…¥æ ¼å¼çš„è§‚æµ‹æ•°æ®
        """
        try:
            # è·å–ç›¸æœºå›¾åƒ
            camera_obs = self._get_camera_observations()
            
            # è·å–æœºå™¨äººçŠ¶æ€
            robot_state = self._get_robot_state()
            
            # æ„å»ºGR00Tè§‚æµ‹å­—å…¸(whole body)
            # observation = {
            #     "video.rs_view": camera_obs["rs_view"],
            #     "state.left_leg": robot_state["left_leg"],
            #     "state.right_leg": robot_state["right_leg"],
            #     "state.waist": robot_state["waist"],
            #     "state.left_arm": robot_state["left_arm"],
            #     "state.right_arm": robot_state["right_arm"], 
            #     "state.left_hand": robot_state["left_hand"],
            #     "state.right_hand": robot_state["right_hand"],
            #     "annotation.human.action.task_description": ["Pick up the red apple and put it on the plate"]
            # }

            observation = {
                "video.rs_view": camera_obs["rs_view"],
                "state.left_arm": robot_state["left_arm"],
                "state.left_hand": robot_state["left_hand"],
                "state.right_arm": robot_state["right_arm"], 
                "state.right_hand": robot_state["right_hand"],
                "annotation.human.action.task_description": ["Pick up the red apple and put it on the plate"]
            }
            
            print(f"æŒ‡ä»¤ï¼š{observation['annotation.human.action.task_description']}")
            #print("è§‚æµ‹æ˜¯ï¼š", observation)
            return observation
            
        except Exception as e:
            print(f"âŒ å‡†å¤‡è§‚æµ‹æ•°æ®æ—¶å‡ºé”™: {e}")
            return self._get_default_observation()
    
    def _get_camera_observations(self) -> Dict[str, np.ndarray]:
        """
        ä»ä»¿çœŸç¯å¢ƒè·å–ç›¸æœºå›¾åƒå¹¶è°ƒæ•´åˆ°GR00TæœŸæœ›çš„å°ºå¯¸
        """
        # if hasattr(self, '_debug_count'):
        #     self._debug_count += 1
        # else:
        #     self._debug_count = 0
        # try:
        #     camera_data = {}
        #     camera_image = None
        #     target_cam_name = 'front_camera' 
            
        #     # ç›´æ¥ä»ç¯å¢ƒåœºæ™¯ä¼ æ„Ÿå™¨(Scene Sensors)è·å– (Isaac Lab æ ‡å‡†æ–¹å¼)
        #     if hasattr(self.env, 'scene') and hasattr(self.env.scene, 'sensors'):
        #         if target_cam_name in self.env.scene.sensors:
        #             sensor = self.env.scene.sensors[target_cam_name]
        #             if hasattr(sensor, 'data') and hasattr(sensor.data, 'output'):
        #                 if 'rgb' in sensor.data.output:
        #                     image_tensor = sensor.data.output['rgb']
                            
        #                     if isinstance(image_tensor, torch.Tensor):
        #                         camera_image = image_tensor.clone().detach().cpu().numpy()
        #                     else:
        #                         camera_image = image_tensor
                                
        #                     # print(f"ğŸ“· è·å–åˆ° {target_cam_name}: {camera_image.shape}")

        #     opencv_image = camera_image.squeeze(axis=0)
        #     cv2.cvtColor(opencv_image, cv2.COLOR_RGB2BGR, opencv_image)
        #     time_stamp = time.strftime('%Y%m%d_%H%M%S') + f'{time.time()%1:.3f}'[1:]
        #     save_dir = os.path.join(os.getcwd(), 'rs_img/')      # å½“å‰ç»ˆç«¯ç›®å½•/rs_img
        #     os.makedirs(save_dir, exist_ok=True)                # æ²¡æœ‰å°±è‡ªåŠ¨å»º

            
        #     file_name = os.path.join(save_dir, f'{self._debug_count}.jpg')
        #     cv2.imwrite(file_name, opencv_image)
        #     if self._debug_count % 10 == 0: 
        #         print('rs_view å·²å†™å…¥', file_name)

        #     # å¤„ç†è·å–åˆ°çš„å›¾åƒ
        #     if camera_image is not None:
        #         # å¦‚æœæ˜¯ RGBA (4é€šé“)ï¼Œå»æ‰ Alpha é€šé“è½¬ä¸º RGB
        #         if camera_image.shape[-1] == 4:
        #             camera_image = camera_image[..., :3]
        #         # print("camera shape:", camera_image.shape)
        #         processed_image = self._process_camera_image(camera_image)
        #         camera_data["rs_view"] = processed_image
        #         return camera_data
                
        #     else:
        #         print(f"âš ï¸ æœªæ‰¾åˆ°ç›¸æœºæ•°æ®: {target_cam_name}")
                
        # except Exception as e:
        #     logging.error(f"âŒ è·å–ç›¸æœºæ•°æ®æ—¶å‡ºé”™: {e}")
        #     import traceback
        #     traceback.print_exc()
        
        # å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›æµ‹è¯•æ•°æ®
        print("âŒ ä½¿ç”¨æµ‹è¯•ç›¸æœºæ•°æ®!")
        return {"rs_view": self._get_test_camera_data()}
    
    def _process_camera_image(self, image: np.ndarray) -> np.ndarray:
        """
        å¤„ç†ç›¸æœºå›¾åƒï¼šè°ƒæ•´å°ºå¯¸ä¸ºGR00TæœŸæœ›çš„å°ºå¯¸ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
        
        Args:
            image: åŸå§‹ç›¸æœºå›¾åƒ
            
        Returns:
            np.ndarray: å¤„ç†åçš„å›¾åƒ
        """
        target_width, target_height = self.image_size
        
        # ç¡®ä¿å›¾åƒæ˜¯NHWCæ ¼å¼ (Batch, Height, Width, Channels)
        if len(image.shape) == 4:
            if image.shape[1] == 3:  # NCHWæ ¼å¼
                # è½¬æ¢ä¸ºNHWC
                image = image.transpose(0, 2, 3, 1)
        elif len(image.shape) == 3:
            # å¦‚æœæ˜¯HWCï¼Œæ·»åŠ batchç»´åº¦
            image = image[np.newaxis, ...]
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸åˆ°ç›®æ ‡å°ºå¯¸
        batch_size = image.shape[0]
        resized_images = np.zeros((batch_size, target_height, target_width, 3), dtype=np.uint8)
        
        for i in range(batch_size):
            img = image[i]
            
            # ç¡®ä¿å›¾åƒæ˜¯uint8ç±»å‹
            if img.dtype != np.uint8:
                if img.max() <= 1.0:  # å‡è®¾æ˜¯0-1èŒƒå›´çš„float
                    img = (img * 255).astype(np.uint8)
                else:
                    img = img.astype(np.uint8)
            
            # è°ƒæ•´å°ºå¯¸åˆ°ç›®æ ‡å°ºå¯¸
            resized_img = cv2.resize(img, (target_width, target_height))
            resized_images[i] = resized_img
        
        return resized_images
    
    def _get_robot_state(self) -> Dict[str, np.ndarray]:
        """
        ä» Isaac Lab çš„ Articulation è·å–çœŸå®çš„æœºå™¨äººå…³èŠ‚çŠ¶æ€
        """
        try:
            # è·å–æœºå™¨äºº Articulation å¯¹è±¡
            robot_articulation = None
            if hasattr(self.env, 'robot'):
                robot_articulation = self.env.robot
            elif hasattr(self.env.scene, 'robot'):
                robot_articulation = self.env.scene['robot']
            elif hasattr(self.env.scene, 'articulations') and len(self.env.scene.articulations) > 0:
                robot_articulation = list(self.env.scene.articulations.values())[0]
            
            if robot_articulation is None:
                raise ValueError("æœªåœ¨ç¯å¢ƒä¸­æ‰¾åˆ°æœºå™¨äºº Articulation å¯¹è±¡")
            
            # è·å–å…³èŠ‚ä½ç½®æ•°æ®
            joint_pos_tensor = robot_articulation.data.joint_pos[0]
            
            if isinstance(joint_pos_tensor, torch.Tensor):
                joint_pos = joint_pos_tensor.detach().cpu().numpy()
            else:
                joint_pos = joint_pos_tensor
            
            joint_pos = joint_pos.astype(np.float32)
            # print("è·å–åˆ°çš„å…³èŠ‚ä½ç½®",joint_pos)
            # æ ¹æ® G129 çš„ 43 ç»´åŠ¨ä½œç©ºé—´æ˜ å°„å…³èŠ‚ç´¢å¼•
            state_data = {}

            # debugä¸“ç”¨
            # joint_pos[15:22] = 1.0
            # joint_pos[22:29] = 2.0
            # joint_pos[29:36] = 3.0
            # joint_pos[36:43] = 4.0

            state_data["left_leg"] = joint_pos[0:6].reshape(1, 6)
            state_data["right_leg"] = joint_pos[6:12].reshape(1, 6)
            # è…°éƒ¨: ç´¢å¼• 12-14 (3ä¸ªå…³èŠ‚)
            state_data["waist"] = joint_pos[12:15].reshape(1, 3)
            # å·¦è‡‚: ç´¢å¼• 15-21 (7ä¸ªå…³èŠ‚)
            state_data["left_arm"] = joint_pos[15:22].reshape(1, 7)
            
            # å³è‡‚: ç´¢å¼• 22-28 (7ä¸ªå…³èŠ‚)
            state_data["right_arm"] = joint_pos[22:29].reshape(1, 7)
            
            # å·¦æ‰‹: ç´¢å¼• 29-35 (7ä¸ªå…³èŠ‚)
            state_data["left_hand"] = joint_pos[29:36].reshape(1, 7)
            
            # å³æ‰‹: ç´¢å¼• 36-42 (7ä¸ªå…³èŠ‚)
            state_data["right_hand"] = joint_pos[36:43].reshape(1, 7)
            
            
            
            # Debug æ‰“å°
            if hasattr(self, '_debug_count'):
                self._debug_count += 1
            else:
                self._debug_count = 0
            
            if self._debug_count % 10 == 0:
                print(f"ğŸ” æœºå™¨äººçŠ¶æ€å·²æ›´æ–°:")
                for key, value in state_data.items():
                    print(f"   {key}: shape={value.shape}, range=[{value.min():.3f}, {value.max():.3f}]")
            
            return state_data
        
        except Exception as e:
            print(f"âŒ ä» Isaac Lab è·å–æœºå™¨äººçŠ¶æ€æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    
    def reset(self):
        """é‡ç½®åŠ¨ä½œæä¾›å™¨çŠ¶æ€"""
        print("ğŸ”„ GR00T Action Provider reset")
        self.action_sequence = None
        self.current_step = 0
    
    def close(self):
        """å…³é—­è¿æ¥"""
        self.session.close()
        print("ğŸ”’ GR00T Action Provider closed")

    def _get_test_camera_data(self) -> np.ndarray:
        """
        ä»å¤–éƒ¨è§†é¢‘æ–‡ä»¶è¯»å–å›¾åƒä½œä¸ºæµ‹è¯•ç›¸æœºæ•°æ®
        
        Returns:
            np.ndarray: å¤„ç†åçš„è§†é¢‘å¸§ï¼Œå½¢çŠ¶ä¸º (1, 480, 640, 3)
        """
        try:
            video_path = None
            if video_path is None:
                # å¦‚æœæ²¡æœ‰é…ç½®è§†é¢‘è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•è§†é¢‘
                default_video = "/home/shenlan/GR00T-VLA/Isaac-GR00T/datasets/g1-pick-apple/videos/chunk-000/observation.images.ego_view/episode_000000.mp4"
                if os.path.exists(default_video):
                    video_path = default_video
                else:
                    # ç”Ÿæˆå½©è‰²æµ‹è¯•å›¾åƒä½œä¸ºå¤‡é€‰
                    print("âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨ç”Ÿæˆçš„æµ‹è¯•å›¾åƒ")
                    return self._generate_test_image()
            
            # åˆå§‹åŒ–è§†é¢‘æ•è·ï¼ˆä½œä¸ºå®ä¾‹å˜é‡ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
            if not hasattr(self, '_test_video_cap'):
                self._test_video_cap = cv2.VideoCapture(video_path)
                self._test_video_frame_count = 0
                
                if not self._test_video_cap.isOpened():
                    print(f"âŒ æ— æ³•æ‰“å¼€æµ‹è¯•è§†é¢‘æ–‡ä»¶: {video_path}")
                    return self._generate_test_image()
                else:
                    print(f"âœ… æˆåŠŸåŠ è½½æµ‹è¯•è§†é¢‘: {video_path}")
            
            # è¯»å–è§†é¢‘å¸§
            ret, frame = self._test_video_cap.read()
            
            if not ret:
                # è§†é¢‘ç»“æŸï¼Œé‡ç½®åˆ°å¼€å¤´
                self._test_video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                ret, frame = self._test_video_cap.read()
                
                if not ret:
                    print("âŒ æ— æ³•ä»è§†é¢‘è¯»å–å¸§ï¼Œä½¿ç”¨æµ‹è¯•å›¾åƒ")
                    return self._generate_test_image()
            
            # è½¬æ¢é¢œè‰²ç©ºé—´ BGR -> RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # è°ƒæ•´å°ºå¯¸åˆ°ç›®æ ‡å°ºå¯¸ (640, 480)
            target_width, target_height = self.image_size
            resized_frame = cv2.resize(frame_rgb, (target_width, target_height))
            
            # æ·»åŠ batchç»´åº¦ (1, H, W, C)
            final_frame = resized_frame[np.newaxis, ...]
            
            # å¸§è®¡æ•°å’Œè°ƒè¯•ä¿¡æ¯
            self._test_video_frame_count += 1
            if self._test_video_frame_count % 30 == 0:  # æ¯30å¸§æ‰“å°ä¸€æ¬¡
                print(f"ğŸ“¹ æµ‹è¯•è§†é¢‘å¸§: {self._test_video_frame_count}, å½¢çŠ¶: {final_frame.shape}")
            
            return final_frame
            
        except Exception as e:
            print(f"âŒ è¯»å–æµ‹è¯•è§†é¢‘æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()